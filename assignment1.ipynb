{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Unsupervised learning\n",
    "### Sanna Hartman Sellæg, Nima Salihzada, Tarjei Åkre Reite\n",
    "\n",
    "\n",
    "This assignment explores a data set with customer data, and attempts to create clusters in the data that can reveal interesting information about different customer profiles.\n",
    "\n",
    "#### Contents\n",
    "0. Load data and imports\n",
    "1. Data preparation\n",
    "2. Choosing number of clusters\n",
    "3. Train k-means models\n",
    "4. Analyze clusters\n",
    "5. Profiling\n",
    "6. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load data and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from kneed import KneeLocator\n",
    "\n",
    "\n",
    "df = pd.read_csv('customer_data_large.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "question_mark_values = (df == '?').sum()\n",
    "\n",
    "if missing_values.sum() > 0 or question_mark_values.sum() > 0:\n",
    "    print(\"Missing or invalid values found.\")\n",
    "else:\n",
    "    print(\"No missing or invalid values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a scaler to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choosing number of clusters\n",
    "\n",
    "Finding the correct number of clusters can be challenging, and to it requires evaluation metrics that do not need labeled data. For this assignment, the following methods will be examined (https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation):\n",
    "1. SSE - Sum of Squared Error\n",
    "2. Silhouette score\n",
    "3. Calinski-Harabasz Index\n",
    "4. Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, try with several different values for k to find out which one(s) are best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_k_to_test = 20\n",
    "\n",
    "sse = []\n",
    "silhouette_scores = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "for i in range(2, number_of_k_to_test):\n",
    "    km = KMeans(n_clusters=i, random_state=19)\n",
    "    km.fit(df_scaled)\n",
    "    sse.append(km.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(df_scaled, km.labels_))\n",
    "    calinski_harabasz.append(calinski_harabasz_score(df_scaled, km.labels_))\n",
    "    davies_bouldin.append(davies_bouldin_score(df_scaled, km.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sum of Squared Error (sse) for each k is then plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(2, number_of_k_to_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('SSE vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the plot of the errors against the number of clusters, the elbow method can be applied. This involves looking for the point where the rate of improvement starts to slow down when adding more clusters, which can look like the elbow of an arm.\n",
    "\n",
    "Visually, it seems like the elbow might be somewhere between 7-10 clusters.\n",
    "\n",
    "Alternatively, an imported library, kneed, has ha function that finds the elbow point. Since this function returns 8, that could indicate that 7-10 clusters was a good estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knee_locator = KneeLocator(range(2, number_of_k_to_test), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee_locator.knee\n",
    "\n",
    "print(f\"The optimal number of clusters is: {optimal_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the silhouette coefficients, where a higher score means that the model has better defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the silhouette coefficients, it looks like the clarity of the clusters stabilize somewhat on the values between 8 and 16. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Calinski-Harabasz Index values are plottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_values, calinski_harabasz, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.title('Calinski-Harabasz Index vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to read anything conclusive from the Calinski-Harabasz values. Typically, higher values mean that the clusters are dense and well separated. It seems like adding more clusters do not improve neither the density or the separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the Davies‑Bouldin Index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_values, davies_bouldin, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.title('Davies-Bouldin Index vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Davies-Bouldin values are lower the better the cluster partition is. There seems to be a local minimum between 8 and 12, before the values go even lower at 16. There is also a very low value at 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of results\n",
    "\n",
    "Four metrics have been used to evaluate how many clusters should be included in the k-means model. A SSE approach indicated that a value around 7-10 could be a good pick, which was further backed up by the silhouette scores, that showed that values between 8 and 16 could be fairly well considerations. Then, not much was learned from the Calinsi-Harabasz scores, but the Davies-Bouldin scores further indicated that a value around 8 clusters could be a good pick.\n",
    "It should be noted that some numbers of clusters gave even better scores in some of the metrics, like 16 in the Davies-Bouldin metrics, but it is probably not desirable to continue with such a high number of clusters. The lowest number of clusters that seem to give sensible scores look like 8-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Averaging out randomness in results\n",
    "\n",
    "As a side note it is useful to consider the fact that the random seed that was chosen (19) could be a statistical outlier, and that the nature of the data in reality is quite different. To explore this, we wrote code that averaged out fifty different random states to see how the results changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_k_to_test = 20\n",
    "\n",
    "sse = []\n",
    "silhouette_scores = []\n",
    "calinski_harabasz = []\n",
    "davies_bouldin = []\n",
    "for i in range(2, number_of_k_to_test):\n",
    "    sse_sum = 0\n",
    "    silhouette_sum = 0\n",
    "    calinski_sum = 0\n",
    "    davies_sum = 0\n",
    "    for r in range(50): # 50 random states\n",
    "        km = KMeans(n_clusters=i, random_state=r)\n",
    "        km.fit(df_scaled)\n",
    "        sse_sum += km.inertia_\n",
    "        silhouette_sum += silhouette_score(df_scaled, km.labels_)\n",
    "        calinski_sum += calinski_harabasz_score(df_scaled, km.labels_)\n",
    "        davies_sum += davies_bouldin_score(df_scaled, km.labels_)\n",
    "    sse.append(sse_sum / 50)\n",
    "    silhouette_scores.append(silhouette_sum / 50)\n",
    "    calinski_harabasz.append(calinski_sum / 50)\n",
    "    davies_bouldin.append(davies_sum / 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = list(range(2, number_of_k_to_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(k_values, sse, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('SSE vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_values, calinski_harabasz, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.title('Calinski-Harabasz Index vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_values, davies_bouldin, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.title('Davies-Bouldin Index vs Number of Clusters')\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpreting the average of fifty random states\n",
    "Perhaps expectedly, the average results are much more smooth than the single simulation based on the random seed 19. Looking at the results it is hard to conclude whether or not the prior estimate of having 8-9 clusters is a good and representative choice. The sum of the squared errors has a very smooth curve, and finding the elbow seems to very much depend on the amount of clusters chosen to include in the simulation. The silhouette scores and the Davies-Bouldin scores seem to indicate that after the first few clusters 2-4 adding more clusters lead to a continuous decrease of performance.\n",
    "\n",
    "To find out what candidates should be examined further, it would be interesting to see how the clusters are distributed if there are both few and many clusters. Therefore, 2 and 8 clusters will be examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the k-means models\n",
    "Now, we make a k-means model for every candidate cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2  = KMeans(n_clusters=2,  random_state=19)\n",
    "km8  = KMeans(n_clusters=8,  random_state=19)\n",
    "\n",
    "km2.fit( df_scaled)\n",
    "km8.fit( df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analyze clusters\n",
    "Now we take a look at the centroids of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers8 = pd.DataFrame(km8.cluster_centers_,columns=df_scaled.columns)\n",
    "centers_unscaled8 = pd.DataFrame(scaler.inverse_transform(centers8),index=centers8.index,columns=centers8.columns)\n",
    "centers_unscaled8.loc['std_dev (standardized)'] = centers8.std()\n",
    "centers_unscaled8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers2 = pd.DataFrame(km2.cluster_centers_,columns=df_scaled.columns)\n",
    "centers_unscaled2 = pd.DataFrame(scaler.inverse_transform(centers2),index=centers2.index,columns=centers2.columns)\n",
    "centers_unscaled2.loc['std_dev (standardized)'] = centers2.std()\n",
    "centers_unscaled2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how much the different categories matter when it comes to dividing into clusters, it can be useful to look at how much a particular category varies from cluster to cluster. To see this, a bottom row has been added to the two dataframes of centroids, showing the standard deviation for each category (calculated on the scaled dataset, for comparison)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Profiling\n",
    "By looking at the centroids, we can see a few interesting trends and attempt to create some customer profiles. Looking at the 2-cluster centroids, it is apparent that we can divide the customers into two main groups, with some key differences.\n",
    "* Group 1 - This group spends much money for every time they go shopping. They tend to not have kids and they often accept campaigns.\n",
    "* Group 2 - This group spends less money and they have more kids. They also do not accept campaigns nearly as much. However, they visit the store on the web more often.\n",
    "\n",
    "It seems like the customers can be divided into two main groups, the shoppers and the non-shoppers. The shoppers will come into the store more often, both on online and to the physical store. The non-shoppers come in more seldom and spend less money. They also do not care as much about the advertising campaigns. They visit the web more often, perhaps indicative of a more price-conscious pattern, where they check often for good deals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(df_scaled)\n",
    "df_pca = pca.transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcolumns = []\n",
    "for i in range (1,df_scaled.shape[1]+1):\n",
    "  lcolumns.append('pc' + str(i))\n",
    "df_pca = pd.DataFrame(df_pca, columns = lcolumns,index=df_scaled.index)\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot( x='pc1', y='pc2',s=60,\n",
    "  data=df_pca,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is a scatterplot of the two most significant principal components in the data. The data shows clearly three clusters of points, which might suggest that there are three clusters in the data. To further explore how this data fits with the clusters found in the k-means approach, we can plot this scatterplots with colors corresponding to the different clusters from k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "models = [km2, km8]\n",
    "titles = ['K-Means (k=3)', 'K-Means (k=4)']\n",
    "\n",
    "for i, (model, ax) in enumerate(zip(models, axes)):\n",
    "    df_pca['Cluster'] = model.labels_\n",
    "    sns.scatterplot(\n",
    "        x='pc1', y='pc2', hue='Cluster', palette='viridis', data=df_pca, ax=ax, legend=None\n",
    "    )\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Principal Component 1')\n",
    "    ax.set_ylabel('Principal Component 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE\n",
    "Applying t-SNE to the data set is helpful to visualize clusterings in the data. The algorithm attempts to project higher dimensional data into lower dimention space. In short, the algorithm tries to make sure that points that are close together in the high dimensional space remain close together in the low-dimension projection. However, the actual structure of the data is not retained, so it is not possible to say anything meaningful about the actual distances between points based on the t-SNE projection. The projection is more useful to visualize clustering in the data.\n",
    "\n",
    "The algorithm depends on a hyperparameter called perplexity, which is a number that states how many of the nearby points a given point should consider when modelling proximity. A lower hyperparameter is better at capturing local patterns, whereas a larger value can capture more global groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "df_tsne = tsne.fit_transform(df_scaled)\n",
    "dicc_tsne = {'comp_1':df_tsne[:,0],'comp_2':df_tsne[:,1]}\n",
    "df_tsne = pd.DataFrame(dicc_tsne,index=df_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(data=df_tsne, x='comp_1',y='comp_2', legend='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for perp in [5,10,20,30,50,100,1000,2000]:\n",
    "  tsne = TSNE(perplexity=perp,random_state=72)\n",
    "  X_tsne = tsne.fit_transform(df_scaled)\n",
    "  dicc_tsne = {'comp_1':X_tsne[:,0],'comp_2':X_tsne[:,1]}\n",
    "  df_tsne = pd.DataFrame(dicc_tsne,index=df_scaled.index)\n",
    "  plt.figure(figsize=(6,6))\n",
    "  plt.title('Perplexity = ' + str(perp))\n",
    "  sns.scatterplot(data=df_tsne, x='comp_1',y='comp_2', legend='full')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various different scatterplots show how the clusters wary with different perplexities. With a very high perplexity it looks like three distinct groups are forming, similarly to the PCA approach. To compare that with the four different candidate clusters from k-means, we can plot the clusters on the scatterplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "models = [km2, km8]\n",
    "titles = ['K-Means (k=2)', 'K-Means (k=8)']\n",
    "\n",
    "tsne = TSNE(perplexity=1000, random_state=72)\n",
    "X_tsne = tsne.fit_transform(df_scaled)\n",
    "df_tsne = pd.DataFrame({\n",
    "    'comp_1': X_tsne[:, 0],\n",
    "    'comp_2': X_tsne[:, 1]\n",
    "}, index=df_scaled.index)\n",
    "\n",
    "\n",
    "for i, (model, ax) in enumerate(zip(models, axes)):\n",
    "    df_tsne['Cluster'] = model.labels_\n",
    "    sns.scatterplot(\n",
    "        x='comp_1', y='comp_2',\n",
    "        hue='Cluster', palette='viridis',\n",
    "        data=df_tsne, ax=ax, legend=None\n",
    "    )\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to PCA, the found clusters do not correspond very well with the k-means clusters. One possible explaination for this is that the clusters that PCA and t-SNE are able to identify are somewhat non-linear of nature, which means that k-means will have a hard time properly defining them.\n",
    "\n",
    "As a last test, we will see how the k-means clusters fit with t-SNE models with lower perplexity and more clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(perplexity=100, random_state=72)\n",
    "X_tsne = tsne.fit_transform(df_scaled)\n",
    "X_tsne_unscaled = tsne.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "models = [km2, km8]\n",
    "titles = ['K-Means (k=2)', 'K-Means (k=8)']\n",
    "df_tsne = pd.DataFrame({\n",
    "    'comp_1': X_tsne[:, 0],\n",
    "    'comp_2': X_tsne[:, 1]\n",
    "}, index=df_scaled.index)\n",
    "\n",
    "\n",
    "for i, (model, ax) in enumerate(zip(models, axes)):\n",
    "    df_tsne['Cluster'] = model.labels_\n",
    "    sns.scatterplot(\n",
    "        x='comp_1', y='comp_2',\n",
    "        hue='Cluster', palette='viridis',\n",
    "        data=df_tsne, ax=ax, legend=None\n",
    "    )\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the clusters in k-means do not correspond very well with the clusters in t-SNE. One possible reason for the mismatch between the clusters in k-means and the clusterings in the t-SNE projection is that the clusters have too complex shapes to be properly captured by k-means."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
